{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1405ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa147d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3355474653.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ENGLISH_STOP_WORDS = {0,1,2,3,4,5,6,7,8,9,a,A,about,above,across,after,again,against,all,almost,alone,along,already,also,although,always,am,among,an,and,another,any,anyone,anything,anywhere,are,aren't,around,as,at,b,B,back,be,became,because,become,becomes,been,before,behind,being,below,between,both,but,by,c,C,can,cannot,can't,could,couldn't,d,D,did,didn't,do,does,doesn't,doing,done,don't,down,during,e,E,each,either,enough,even,ever,every,everyone,everything,everywhere,f,F,few,find,first,for,four,from,full,further,g,G,get,give,go,h,H,had,hadn't,has,hasn't,have,haven't,having,he,he'd,he'll,her,here,here's,hers,herself,he's,him,himself,his,how,however,how's,i,I,i'd,if,i'll,i'm,in,interest,into,is,isn't,it,it's,its,itself,i've,j,J,k,K,keep,l,L,last,least,less,let's,m,M,made,many,may,me,might,more,most,mostly,much,must,mustn't,my,myself,n,N,never,next,no,nobody,noone,nor,not,nothing,now,nowhere,o,O,of,off,often,on,once,one,only,or,other,others,ought,our,ours,ourselves,out,over,own,p,P,part,per,perhaps,put,q,Q,r,R,rather,s,S,same,see,seem,seemed,seeming,seems,several,shan't,she,she'd,she'll,she's,should,shouldn't,show,side,since,so,some,someone,something,somewhere,still,such,t,T,take,than,that,that's,the,their,theirs,them,themselves,then,there,therefore,there's,these,they,they'd,they'll,they're,they've,this,those,though,three,through,thus,to,together,too,toward,two,u,U,under,until,up,upon,us,v,V,very,w,W,was,wasn't,we,we'd,we'll,well,we're,were,weren't,we've,what,what's,when,when's,where,where's,whether,which,while,who,whole,whom,who's,whose,why,why's,will,with,within,without,won't,would,wouldn't,x,X,y,Y,yet,you,you'd,you'll,your,you're,yours,yourself,yourselves,you've,z,Z}\u001b[0m\n\u001b[0m                                                                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1809e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa9103e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>RT @3novices: Angela Merkel calls Russian inva...</td>\n",
       "      <td>LuisaRagni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @DominoDataLab: Learn from the creators of ...</td>\n",
       "      <td>XeronBot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @DominoDataLab: Learn from the creators of ...</td>\n",
       "      <td>grantho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>RT @ragipsoylu: Current control of territory i...</td>\n",
       "      <td>Data_Science_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technically, if 442.2% of the population is in...</td>\n",
       "      <td>sporksys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text   \n",
       "0   neutral  RT @3novices: Angela Merkel calls Russian inva...  \\\n",
       "1  negative  RT @DominoDataLab: Learn from the creators of ...   \n",
       "2  negative  RT @DominoDataLab: Learn from the creators of ...   \n",
       "3   neutral  RT @ragipsoylu: Current control of territory i...   \n",
       "4   neutral  Technically, if 442.2% of the population is in...   \n",
       "\n",
       "              user  \n",
       "0       LuisaRagni  \n",
       "1         XeronBot  \n",
       "2          grantho  \n",
       "3  Data_Science_11  \n",
       "4         sporksys  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the top rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ecbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2 = pd.read_csv(\"archive/tweet_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cebbfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>RT @3novices: Angela Merkel calls Russian inva...</td>\n",
       "      <td>LuisaRagni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @DominoDataLab: Learn from the creators of ...</td>\n",
       "      <td>XeronBot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>RT @DominoDataLab: Learn from the creators of ...</td>\n",
       "      <td>grantho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>RT @ragipsoylu: Current control of territory i...</td>\n",
       "      <td>Data_Science_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technically, if 442.2% of the population is in...</td>\n",
       "      <td>sporksys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>neutral</td>\n",
       "      <td>RT @Datasciencectrl: By Nikita Godse üë®\\nIoT in...</td>\n",
       "      <td>ravincap25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>neutral</td>\n",
       "      <td>RT @NASAKennedy: The countdown is ON! üöÄ‚åõÔ∏è\\n\\n@...</td>\n",
       "      <td>MESSAloveithere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @VPrasadMDMPH: Wow, I see the CDC now recom...</td>\n",
       "      <td>judybats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>positive</td>\n",
       "      <td>RT @VPrasadMDMPH: Wow, I see the CDC now recom...</td>\n",
       "      <td>stephen7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>positive</td>\n",
       "      <td>We are thrilled to uncover our logo for the Da...</td>\n",
       "      <td>DSC_VITB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text   \n",
       "0    neutral  RT @3novices: Angela Merkel calls Russian inva...  \\\n",
       "1   negative  RT @DominoDataLab: Learn from the creators of ...   \n",
       "2   negative  RT @DominoDataLab: Learn from the creators of ...   \n",
       "3    neutral  RT @ragipsoylu: Current control of territory i...   \n",
       "4    neutral  Technically, if 442.2% of the population is in...   \n",
       "..       ...                                                ...   \n",
       "95   neutral  RT @Datasciencectrl: By Nikita Godse üë®\\nIoT in...   \n",
       "96   neutral  RT @NASAKennedy: The countdown is ON! üöÄ‚åõÔ∏è\\n\\n@...   \n",
       "97  positive  RT @VPrasadMDMPH: Wow, I see the CDC now recom...   \n",
       "98  positive  RT @VPrasadMDMPH: Wow, I see the CDC now recom...   \n",
       "99  positive  We are thrilled to uncover our logo for the Da...   \n",
       "\n",
       "               user  \n",
       "0        LuisaRagni  \n",
       "1          XeronBot  \n",
       "2           grantho  \n",
       "3   Data_Science_11  \n",
       "4          sporksys  \n",
       "..              ...  \n",
       "95       ravincap25  \n",
       "96  MESSAloveithere  \n",
       "97         judybats  \n",
       "98      stephen7778  \n",
       "99         DSC_VITB  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61f163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         40000 non-null  int64 \n",
      " 1   sentiment      40000 non-null  object\n",
      " 2   author         40000 non-null  object\n",
      " 3   text           39934 non-null  object\n",
      " 4   old_text       40000 non-null  object\n",
      " 5   aux_id         40000 non-null  object\n",
      " 6   new_sentiment  31395 non-null  object\n",
      " 7   selected_text  27767 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419413a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>old_text</th>\n",
       "      <th>aux_id</th>\n",
       "      <th>new_sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>p1000000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>c811396dc2</td>\n",
       "      <td>negative</td>\n",
       "      <td>headache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>9063631ab1</td>\n",
       "      <td>negative</td>\n",
       "      <td>gloomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>2a815f151d</td>\n",
       "      <td>positive</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>We want to trade with someone who has Houston...</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>82565a56d3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>We want to trade with someone who has Houston ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                                text  \\\n",
       "0   i know  i was listenin to bad habit earlier a...   \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2                Funeral ceremony...gloomy friday...   \n",
       "3               wants to hang out with friends SOON!   \n",
       "4   We want to trade with someone who has Houston...   \n",
       "\n",
       "                                            old_text       aux_id  \\\n",
       "0  @tiffanylue i know  i was listenin to bad habi...  p1000000000   \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...   c811396dc2   \n",
       "2                Funeral ceremony...gloomy friday...   9063631ab1   \n",
       "3               wants to hang out with friends SOON!   2a815f151d   \n",
       "4  @dannycastillo We want to trade with someone w...   82565a56d3   \n",
       "\n",
       "  new_sentiment                                      selected_text  \n",
       "0           NaN                                                NaN  \n",
       "1      negative                                           headache  \n",
       "2      negative                                             gloomy  \n",
       "3      positive               wants to hang out with friends SOON!  \n",
       "4       neutral  We want to trade with someone who has Houston ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8888af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train, test = train_test_split(data, test_size = 0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2047bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80, 3), (20, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the shape of train and test split.\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2db5ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TF-IDF vectorizer object\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase= True, max_features=1000, stop_words=ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba5413ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'would', 'can', 'keep', 'if', 'con', 'beyond', 'therein', 'made', 'whether', 'describe', 'well', 'inc', 'alone', 'thence', 'couldnt', 'among', 'next', 'their', 'un', 'against', 'all', 'will', 'very', 'only', 'where', 'thus', 'off', 'serious', 'was', 'has', 'thereby', 'many', 'ie', 'is', 'because', 'do', 'formerly', 'around', 'often', 'thick', 'on', 'that', 'might', 'sometimes', 'through', 'also', 'been', 'cant', 'ever', 'they', 'throughout', 'could', 'over', 'back', 'may', 'out', 'none', 'hereby', 'any', 'enough', 'besides', 'hence', 'amongst', 'either', 'herein', 'but', 'six', 'nor', 'within', 'towards', 'hereafter', 'yours', 'herself', 'found', 'thereafter', 'are', 'she', 'too', 'else', 'fifty', 'each', 'of', 'be', 'four', 'sixty', 'somehow', 'here', 'had', 'without', 'bottom', 'nowhere', 'still', 'into', 'whoever', 'detail', 'first', 'co', 'about', 'he', 'de', 'already', 'an', 'thereupon', 'top', 'how', 'give', 'my', 'ourselves', 'him', 'bill', 'cry', 'by', 'call', 'a', 'himself', 'whole', 'hundred', 'ours', 'such', 'whereby', 'anything', 'the', 'eleven', 'those', 'except', 'between', 'whatever', 'interest', 'above', 'this', 'whenever', 'afterwards', 'latterly', 'otherwise', 'most', 'nobody', 'eight', 'however', 'five', 'anyway', 'whence', 'ltd', 'his', 'becoming', 'must', 'system', 'three', 'something', 'when', 're', 'together', 'somewhere', 'while', 'find', 'your', 'seems', 'nothing', 'everyone', 'though', 'whom', 'forty', 'further', 'it', 'least', 'whither', 'there', 'cannot', 'onto', 'its', 'yourself', 'few', 'under', 'one', 'no', 'everywhere', 'her', 'during', 'anyhow', 'former', 'themselves', 'hers', 'yet', 'always', 'at', 'seem', 'therefore', 'then', 'amoungst', 'with', 'now', 'again', 'as', 'more', 'and', 'mostly', 'along', 'being', 'sometime', 'which', 'mill', 'twenty', 'after', 'perhaps', 'for', 'sincere', 'eg', 'anywhere', 'since', 'so', 'up', 'everything', 'nine', 'wherever', 'who', 'wherein', 'below', 'became', 'am', 'whereas', 'whereafter', 'should', 'from', 'via', 'becomes', 'nevertheless', 'side', 'rather', 'another', 'almost', 'take', 'every', 'others', 'although', 'thru', 'in', 'per', 'less', 'hereupon', 'become', 'have', 'empty', 'not', 'to', 'why', 'beside', 'get', 'fire', 'ten', 'what', 'upon', 'go', 'namely', 'me', 'never', 'name', 'toward', 'noone', 'you', 'once', 'amount', 'until', 'moreover', 'front', 'twelve', 'two', 'myself', 'mine', 'show', 'anyone', 'whose', 'etc', 'done', 'down', 'behind', 'some', 'yourselves', 'them', 'whereupon', 'us', 'or', 'before', 'put', 'move', 'other', 'last', 'full', 'than', 'own', 'latter', 'seeming', 'were', 'across', 'our', 'fifteen', 'meanwhile', 'see', 'these', 'elsewhere', 'indeed', 'neither', 'beforehand', 'itself', 'we', 'same', 'someone', 'fill', 'both', 'please', 'i', 'several', 'third', 'due', 'thin', 'even', 'seemed', 'hasnt', 'much', 'part'}) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fit the object with the training data tweets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/study/Work/SentimentAnalysisML/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2094\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn vocabulary and idf from training set.\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m \n\u001b[1;32m   2081\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2096\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n",
      "File \u001b[0;32m~/Documents/study/Work/SentimentAnalysisML/venv/lib/python3.11/site-packages/sklearn/base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    593\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 600\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/study/Work/SentimentAnalysisML/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'would', 'can', 'keep', 'if', 'con', 'beyond', 'therein', 'made', 'whether', 'describe', 'well', 'inc', 'alone', 'thence', 'couldnt', 'among', 'next', 'their', 'un', 'against', 'all', 'will', 'very', 'only', 'where', 'thus', 'off', 'serious', 'was', 'has', 'thereby', 'many', 'ie', 'is', 'because', 'do', 'formerly', 'around', 'often', 'thick', 'on', 'that', 'might', 'sometimes', 'through', 'also', 'been', 'cant', 'ever', 'they', 'throughout', 'could', 'over', 'back', 'may', 'out', 'none', 'hereby', 'any', 'enough', 'besides', 'hence', 'amongst', 'either', 'herein', 'but', 'six', 'nor', 'within', 'towards', 'hereafter', 'yours', 'herself', 'found', 'thereafter', 'are', 'she', 'too', 'else', 'fifty', 'each', 'of', 'be', 'four', 'sixty', 'somehow', 'here', 'had', 'without', 'bottom', 'nowhere', 'still', 'into', 'whoever', 'detail', 'first', 'co', 'about', 'he', 'de', 'already', 'an', 'thereupon', 'top', 'how', 'give', 'my', 'ourselves', 'him', 'bill', 'cry', 'by', 'call', 'a', 'himself', 'whole', 'hundred', 'ours', 'such', 'whereby', 'anything', 'the', 'eleven', 'those', 'except', 'between', 'whatever', 'interest', 'above', 'this', 'whenever', 'afterwards', 'latterly', 'otherwise', 'most', 'nobody', 'eight', 'however', 'five', 'anyway', 'whence', 'ltd', 'his', 'becoming', 'must', 'system', 'three', 'something', 'when', 're', 'together', 'somewhere', 'while', 'find', 'your', 'seems', 'nothing', 'everyone', 'though', 'whom', 'forty', 'further', 'it', 'least', 'whither', 'there', 'cannot', 'onto', 'its', 'yourself', 'few', 'under', 'one', 'no', 'everywhere', 'her', 'during', 'anyhow', 'former', 'themselves', 'hers', 'yet', 'always', 'at', 'seem', 'therefore', 'then', 'amoungst', 'with', 'now', 'again', 'as', 'more', 'and', 'mostly', 'along', 'being', 'sometime', 'which', 'mill', 'twenty', 'after', 'perhaps', 'for', 'sincere', 'eg', 'anywhere', 'since', 'so', 'up', 'everything', 'nine', 'wherever', 'who', 'wherein', 'below', 'became', 'am', 'whereas', 'whereafter', 'should', 'from', 'via', 'becomes', 'nevertheless', 'side', 'rather', 'another', 'almost', 'take', 'every', 'others', 'although', 'thru', 'in', 'per', 'less', 'hereupon', 'become', 'have', 'empty', 'not', 'to', 'why', 'beside', 'get', 'fire', 'ten', 'what', 'upon', 'go', 'namely', 'me', 'never', 'name', 'toward', 'noone', 'you', 'once', 'amount', 'until', 'moreover', 'front', 'twelve', 'two', 'myself', 'mine', 'show', 'anyone', 'whose', 'etc', 'done', 'down', 'behind', 'some', 'yourselves', 'them', 'whereupon', 'us', 'or', 'before', 'put', 'move', 'other', 'last', 'full', 'than', 'own', 'latter', 'seeming', 'were', 'across', 'our', 'fifteen', 'meanwhile', 'see', 'these', 'elsewhere', 'indeed', 'neither', 'beforehand', 'itself', 'we', 'same', 'someone', 'fill', 'both', 'please', 'i', 'several', 'third', 'due', 'thin', 'even', 'seemed', 'hasnt', 'much', 'part'}) instead."
     ]
    }
   ],
   "source": [
    "# fit the object with the training data tweets\n",
    "tfidf_vectorizer.fit(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a29cba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the train and test data\n",
    "train_idf = tfidf_vectorizer.transform(train.old_text)\n",
    "test_idf  = tfidf_vectorizer.transform(test.old_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d41dcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the object of Logistic Regression Model\n",
    "model_LR = RandomForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e09b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = LogisticRegression(solver='lbfgs', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc42bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model with the training data\n",
    "model_LR.fit(train_idf, train.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec46347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the label on the traning data\n",
    "predict_train = model_LR.predict(train_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51158ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the model on the test data\n",
    "predict_test = model_LR.predict(test_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19967f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3971875"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f1 score on train data\n",
    "f1_score(y_true= train.sentiment, y_pred= predict_train, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2436d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35325"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true= test.sentiment, y_pred= predict_test, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "222ed377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the stages of the pipeline\n",
    "pipeline = Pipeline(steps= [('tfidf', TfidfVectorizer(lowercase=True,\n",
    "                                                      max_features=1000,\n",
    "                                                      stop_words= ENGLISH_STOP_WORDS)),\n",
    "                            ('model', LogisticRegression(solver='lbfgs', max_iter=1000))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69c52451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=1000,\n",
       "                                 stop_words=frozenset({'a', 'about', 'above',\n",
       "                                                       'across', 'after',\n",
       "                                                       'afterwards', 'again',\n",
       "                                                       'against', 'all',\n",
       "                                                       'almost', 'alone',\n",
       "                                                       'along', 'already',\n",
       "                                                       'also', 'although',\n",
       "                                                       'always', 'am', 'among',\n",
       "                                                       'amongst', 'amoungst',\n",
       "                                                       'amount', 'an', 'and',\n",
       "                                                       'another', 'any',\n",
       "                                                       'anyhow', 'anyone',\n",
       "                                                       'anything', 'anyway',\n",
       "                                                       'anywhere', ...}))),\n",
       "                ('model', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the pipeline model with the training data                            \n",
    "pipeline.fit(train.old_text, train.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3499e91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['love'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample tweet\n",
    "text = [\"i love her on the outside but hate her from the inside\"]\n",
    "\n",
    "# predict the label using the pipeline\n",
    "pipeline.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d32ad14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_classification.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# dump the pipeline model\n",
    "dump(pipeline, filename=\"text_classification.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4be0927c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>old_text</th>\n",
       "      <th>aux_id</th>\n",
       "      <th>new_sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1956968477</td>\n",
       "      <td>worry</td>\n",
       "      <td>xxxPEACHESxxx</td>\n",
       "      <td>Re-pinging : why didn`t you go to prom? BC my ...</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "      <td>a610d6b25b</td>\n",
       "      <td>negative</td>\n",
       "      <td>didn`t like my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1956968636</td>\n",
       "      <td>worry</td>\n",
       "      <td>mcsleazy</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "      <td>2dfbe0b7fb</td>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1956969531</td>\n",
       "      <td>worry</td>\n",
       "      <td>dudeitsmanda</td>\n",
       "      <td>Choked on her retainers</td>\n",
       "      <td>Choked on her retainers</td>\n",
       "      <td>133109505a</td>\n",
       "      <td>negative</td>\n",
       "      <td>Choked on her retainers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1956971473</td>\n",
       "      <td>worry</td>\n",
       "      <td>LCJ82</td>\n",
       "      <td>lady gaga tweeted about not being impressed b...</td>\n",
       "      <td>@PerezHilton lady gaga tweeted about not being...</td>\n",
       "      <td>23f0f2d1f3</td>\n",
       "      <td>negative</td>\n",
       "      <td>not being impressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1956971981</td>\n",
       "      <td>worry</td>\n",
       "      <td>andreagauster</td>\n",
       "      <td>oh too bad! I hope it gets better. I`ve been ...</td>\n",
       "      <td>@raaaaaaek oh too bad! I hope it gets better. ...</td>\n",
       "      <td>ce2c823958</td>\n",
       "      <td>neutral</td>\n",
       "      <td>oh too bad! I hope it gets better. I`ve been h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39936</th>\n",
       "      <td>1753903426</td>\n",
       "      <td>worry</td>\n",
       "      <td>carastjohn</td>\n",
       "      <td>tomorrow is going to be sooo awkward &amp; embaras...</td>\n",
       "      <td>tomorrow is going to be sooo awkward &amp;amp; emb...</td>\n",
       "      <td>09140327f6</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39938</th>\n",
       "      <td>1753903505</td>\n",
       "      <td>worry</td>\n",
       "      <td>primatage</td>\n",
       "      <td>hey! negative on the primatech, this handle`s...</td>\n",
       "      <td>@icebergstorm hey! negative on the primatech, ...</td>\n",
       "      <td>7f7bd175fb</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39941</th>\n",
       "      <td>1753903578</td>\n",
       "      <td>worry</td>\n",
       "      <td>somemandy</td>\n",
       "      <td>sure. But be careful also of making statement...</td>\n",
       "      <td>@PH7S sure. But be careful also of making stat...</td>\n",
       "      <td>a0b022f817</td>\n",
       "      <td>positive</td>\n",
       "      <td>But be careful also of making statements that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39956</th>\n",
       "      <td>1753903987</td>\n",
       "      <td>worry</td>\n",
       "      <td>nadszy</td>\n",
       "      <td>How Do You Sleep - Jesse McCartney</td>\n",
       "      <td>How Do You Sleep - Jesse McCartney</td>\n",
       "      <td>f63400d9fb</td>\n",
       "      <td>neutral</td>\n",
       "      <td>How Do You Sleep - Jesse McCartney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39978</th>\n",
       "      <td>1753904868</td>\n",
       "      <td>worry</td>\n",
       "      <td>theknickermafia</td>\n",
       "      <td>bloody Feds, they lost last statement and r h...</td>\n",
       "      <td>@givemestrength bloody Feds, they lost last st...</td>\n",
       "      <td>cf405b31c1</td>\n",
       "      <td>negative</td>\n",
       "      <td>bloody Feds,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8459 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID sentiment           author  \\\n",
       "5      1956968477     worry    xxxPEACHESxxx   \n",
       "7      1956968636     worry         mcsleazy   \n",
       "11     1956969531     worry     dudeitsmanda   \n",
       "18     1956971473     worry            LCJ82   \n",
       "20     1956971981     worry    andreagauster   \n",
       "...           ...       ...              ...   \n",
       "39936  1753903426     worry       carastjohn   \n",
       "39938  1753903505     worry        primatage   \n",
       "39941  1753903578     worry        somemandy   \n",
       "39956  1753903987     worry           nadszy   \n",
       "39978  1753904868     worry  theknickermafia   \n",
       "\n",
       "                                                    text  \\\n",
       "5      Re-pinging : why didn`t you go to prom? BC my ...   \n",
       "7                   Hmmm. http://www.djhero.com/ is down   \n",
       "11                               Choked on her retainers   \n",
       "18      lady gaga tweeted about not being impressed b...   \n",
       "20      oh too bad! I hope it gets better. I`ve been ...   \n",
       "...                                                  ...   \n",
       "39936  tomorrow is going to be sooo awkward & embaras...   \n",
       "39938   hey! negative on the primatech, this handle`s...   \n",
       "39941   sure. But be careful also of making statement...   \n",
       "39956                 How Do You Sleep - Jesse McCartney   \n",
       "39978   bloody Feds, they lost last statement and r h...   \n",
       "\n",
       "                                                old_text      aux_id  \\\n",
       "5      Re-pinging @ghostridah14: why didn't you go to...  a610d6b25b   \n",
       "7                   Hmmm. http://www.djhero.com/ is down  2dfbe0b7fb   \n",
       "11                               Choked on her retainers  133109505a   \n",
       "18     @PerezHilton lady gaga tweeted about not being...  23f0f2d1f3   \n",
       "20     @raaaaaaek oh too bad! I hope it gets better. ...  ce2c823958   \n",
       "...                                                  ...         ...   \n",
       "39936  tomorrow is going to be sooo awkward &amp; emb...  09140327f6   \n",
       "39938  @icebergstorm hey! negative on the primatech, ...  7f7bd175fb   \n",
       "39941  @PH7S sure. But be careful also of making stat...  a0b022f817   \n",
       "39956                 How Do You Sleep - Jesse McCartney  f63400d9fb   \n",
       "39978  @givemestrength bloody Feds, they lost last st...  cf405b31c1   \n",
       "\n",
       "      new_sentiment                                      selected_text  \n",
       "5          negative                                     didn`t like my  \n",
       "7          negative                                                NaN  \n",
       "11         negative                            Choked on her retainers  \n",
       "18         negative                                not being impressed  \n",
       "20          neutral  oh too bad! I hope it gets better. I`ve been h...  \n",
       "...             ...                                                ...  \n",
       "39936       neutral                                                NaN  \n",
       "39938      negative                                           negative  \n",
       "39941      positive  But be careful also of making statements that ...  \n",
       "39956       neutral                 How Do You Sleep - Jesse McCartney  \n",
       "39978      negative                                       bloody Feds,  \n",
       "\n",
       "[8459 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[data2.sentiment == 'worry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca70740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
